// `get_intrinsic` is a global because it itself
// needs to be used to access module exports
// TODO allow non-intrinsic overloads
operator(.) :: 20 __get

operator(' _) :: 30 MASS.quote
operator(_ ') :: 30 MASS.unquote

// TODO allow non-intrinsic overloads
operator(=) :: 1 MASS.assign
operator(_ .*) :: 20 MASS.dereference

operator(:) :: 2 MASS.typed_symbol

operator(@ _) :: 20 MASS.eval
operator(. _) :: 30 MASS.named_accessor

operator(==) :: 7 'equal
operator(!=) :: 7 'not_equal

operator(<) :: 8 'less
operator(>) :: 8 'greater
operator(<=) :: 8 'less_equal
operator(>=) :: 8 'greater_equal

operator(+) :: 10 'add
operator(-) :: 10 'subtract

operator(*) :: 15 'multiply
operator(/) :: 15 'divide
operator(%) :: 15 'remainder

operator(<<) :: 15 'logical_shift_left
operator(>>) :: 15 'logical_shift_right
operator(|) :: 15 'bitwise_or
operator(&) :: 15 'bitwise_and

operator(- _) :: 16 'negate
operator(& _) :: 16 'pointer_to


import :: fn(path : String) -> (MASS.Module) MASS.import
cast :: fn(type : Type, value) -> _ MASS.cast
startup :: fn(callback : fn() -> ()) -> () MASS.startup

Void :: type_of(())
make_void :: fn(allocator : &Allocator, source_range : &MASS.Source_Range) -> (&MASS.Value) {
  value : &MASS.Value = allocate(allocator, MASS.Value)
  value.source_range = source_range.*
  value.descriptor = Void
  value.storage.tag = MASS.Storage_Tag.None
  value.storage.bit_size = value.descriptor.bit_size
  value
}

debugger :: fn() -> () intrinsic {
  lazy_value_proc :: fn(
    context : &MASS.Context,
    builder : &MASS.Function_Builder,
    expected_result : &MASS.Expected_Result,
    source_range : &MASS.Source_Range,
    payload : &Void
  ) -> (&MASS.Value) {
    {
      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Location
      instruction.Location.source_range = source_range.*
    }

    {
      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Bytes
      instruction.Bytes.memory.0 = cast(i8, 0xcc)
      instruction.Bytes.length = cast(i8, 1)
      MASS.push_instruction(&builder.code_block, instruction);
    }

    make_void(context.allocator, source_range)
  }


  lazy_value := allocate(context.allocator, MASS.Lazy_Value)
  lazy_value.epoch = parser.epoch
  lazy_value.descriptor = Void
  lazy_value.proc = lazy_value_proc
  lazy_value.payload = 0

  meta :: import("std/meta")
  meta.static_value(context.compilation, lazy_value, arguments.source_range)
}


Symbol :: MASS.Symbol
true :: cast(bool, 1)
false :: cast(bool, 0)

assert :: fn(condition : bool, message := "Assertion failed\n") -> () {
  // TODO add something like MASS.intrinsic_assert or MASS.intrinsic_print
  //      that would be safe to call without a risk or circular dependencies
  // TODO also detect these kind of dependencies better
  //io :: import("std/io")
  if condition then {} else {
    //io.print(message)
    debugger()
  }
}

static_assert :: fn(condition : bool, message : String = "") => () MASS.static_assert

apply :: fn(symbol :: 'module, curly : MASS.Ast_Block) -> (MASS.Module) MASS.inline_module
apply :: fn(symbol :: 'exports, curly : MASS.Group_Square) -> () MASS.exports
apply :: fn(symbol :: 'intrinsic, curly : MASS.Ast_Block) -> (MASS.Intrinsic_Proc) MASS.intrinsic
apply :: fn(symbol :: 'c_struct, curly : MASS.Group_Square) -> (Type) MASS.c_struct

// These need to be an overload of `apply` instead a regular function because
// it needs to parse the expression inside with the runtime epoch even in the case
// when it is called from a constant context
apply :: fn(symbol :: 'type_of, paren : MASS.Group_Paren) -> (Type) MASS.type_of
apply :: fn(symbol :: 'size_of, paren : MASS.Group_Paren) -> (i64) MASS.size_of

apply :: fn(x, paren : MASS.Group_Paren) -> _ MASS.call

pointer_to :: fn(type : Type) => (Type) MASS.pointer_to_type
pointer_to :: fn(x) -> (pointer_to(x)) MASS.pointer_to

make_arithmetic_operations_module :: macro(type : Type) {
  module {
    add :: fn(x : type, y : type) -> (type) MASS.integer_add
    subtract :: fn(x : type, y : type) -> (type) MASS.integer_subtract
    multiply :: fn(x : type, y : type) -> (type) MASS.integer_multiply
    divide :: fn(x : type, y : type) -> (type) MASS.integer_divide
    remainder :: fn(x : type, y : type) -> (type) MASS.integer_remainder
    negate :: fn(x : type) -> (type) { 0 - x }

    less :: fn(x : type, y : type) -> (bool) MASS.integer_less
    less_equal :: fn(x : type, y : type) -> (bool) MASS.integer_less_equal
    greater :: fn(x : type, y : type) -> (bool) MASS.integer_greater
    greater_equal :: fn(x : type, y : type) -> (bool) MASS.integer_greater_equal
    equal :: fn(x : type, y : type) -> (bool) MASS.integer_equal
    not_equal :: fn(x : type, y : type) -> (bool) MASS.integer_not_equal
  }
}

using make_arithmetic_operations_module(s8)
using make_arithmetic_operations_module(s16)
using make_arithmetic_operations_module(s32)
using make_arithmetic_operations_module(s64)
using make_arithmetic_operations_module(u8)
using make_arithmetic_operations_module(u16)
using make_arithmetic_operations_module(u32)
using make_arithmetic_operations_module(u64)

add_or_subtract :: fn(
  context : &MASS.Context,
  parser : &MASS.Parser,
  arguments : MASS.Value_View,
  op_code : i8
) -> (&MASS.Value) {
  meta :: import("std/meta")

  Payload :: c_struct [ lhs : &MASS.Value, rhs : &MASS.Value, op_code : i8 ]

  lazy_value_proc :: fn(
    context : &MASS.Context,
    builder : &MASS.Function_Builder,
    expected_result : &MASS.Expected_Result,
    source_range : &MASS.Source_Range,
    raw_payload : &Void
  ) -> (&MASS.Value) {
    payload := cast(&Payload, raw_payload)
    temp_lhs_storage := MASS.storage_register_temp(builder, i64.bit_size);
    temp_lhs_register := temp_lhs_storage.Register.index
    expected_lhs := MASS.expected_result_exact(i64, temp_lhs_storage)

    temp_lhs := MASS.value_force(context, builder, &expected_lhs, payload.lhs)

    if context.compilation.result.tag != MASS.Result_Tag.Success then {
      return 0
    } else {}

    temp_rhs_storage := MASS.storage_register_temp(builder, i64.bit_size);
    temp_rhs_register := temp_rhs_storage.Register.index
    expected_rhs := MASS.expected_result_exact(i64, temp_rhs_storage)

    temp_rhs := MASS.value_force(context, builder, &expected_rhs, payload.lhs)

    if context.compilation.result.tag != MASS.Result_Tag.Success then {
      return 0
    } else {}

    {
      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Location
      instruction.Location.source_range = source_range.*
    }

    {
      mod_reg_rm := MASS.mod_reg_rm(temp_lhs_register, temp_rhs_register)

      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Bytes
      instruction.Bytes.memory.0 = cast(i8, 0x48)
      instruction.Bytes.memory.1 = payload.op_code
      instruction.Bytes.memory.2 = cast(i8, mod_reg_rm)
      instruction.Bytes.length = cast(i8, 3)
      MASS.push_instruction(&builder.code_block, instruction);
    }

    MASS.register_release(builder, temp_rhs_register);

    temp_lhs
  }

  payload := allocate(context.allocator, Payload)
  payload.* = [arguments.values.0, arguments.values.1, op_code]

  lazy_value := allocate(context.allocator, MASS.Lazy_Value)
  lazy_value.epoch = parser.epoch
  lazy_value.descriptor = i64
  lazy_value.proc = lazy_value_proc
  lazy_value.payload = payload

  meta.static_value(context.compilation, lazy_value, arguments.source_range)
}

add :: fn(x : i64, y : i64) -> (i64) intrinsic {
  add_or_subtract(context, parser, arguments, cast(i8, 0x01))
}

subtract :: fn(x : i64, y : i64) -> (i64) intrinsic {
  add_or_subtract(context, parser, arguments, cast(i8, 0x29))
}

unsigned_less :: fn(x : i64, y : i64) -> (bool) intrinsic {
  meta :: import("std/meta")

  Payload :: c_struct [ lhs : &MASS.Value, rhs : &MASS.Value ]

  lazy_value_proc :: fn(
    context : &MASS.Context,
    builder : &MASS.Function_Builder,
    expected_result : &MASS.Expected_Result,
    source_range : &MASS.Source_Range,
    raw_payload : &Void
  ) -> (&MASS.Value) {
    payload := cast(&Payload, raw_payload)
    // TODO figure out why this fails
    //Storage temp_lhs_storage = storage_register_temp(builder, descriptor->bit_size);

    temp_lhs_register := MASS.register_acquire_temp(builder)
    temp_lhs_storage : MASS.Storage
    {
      temp_lhs_storage.flags = MASS.Storage_Flags.Temporary
      temp_lhs_storage.tag = MASS.Storage_Tag.Register
      temp_lhs_storage.bit_size = [64]
      temp_lhs_storage.Register.index = temp_lhs_register
      temp_lhs_storage.Register.packed = cast(i16, 0)
      temp_lhs_storage.Register.offset_in_bits = cast(i16, 0)
    }
    expected_lhs : MASS.Expected_Result
    {
      expected_lhs.tag = MASS.Expected_Result_Tag.Exact
      expected_lhs.Exact.descriptor = i64
      expected_lhs.Exact.storage = temp_lhs_storage
    }

    temp_lhs := MASS.value_force(context, builder, &expected_lhs, payload.lhs)

    if context.compilation.result.tag != MASS.Result_Tag.Success then {
      return 0
    } else {}


    temp_rhs_register := MASS.register_acquire_temp(builder)
    temp_rhs_storage : MASS.Storage
    {
      temp_rhs_storage.flags = MASS.Storage_Flags.Temporary
      temp_rhs_storage.tag = MASS.Storage_Tag.Register
      temp_rhs_storage.bit_size = [64]
      temp_rhs_storage.Register.index = temp_rhs_register
      temp_rhs_storage.Register.packed = cast(i16, 0)
      temp_rhs_storage.Register.offset_in_bits = cast(i16, 0)
    }
    expected_rhs : MASS.Expected_Result
    {
      expected_rhs.tag = MASS.Expected_Result_Tag.Exact
      expected_rhs.Exact.descriptor = i64
      expected_rhs.Exact.storage = temp_rhs_storage
    }

    temp_rhs := MASS.value_force(context, builder, &expected_rhs, payload.rhs)

    if context.compilation.result.tag != MASS.Result_Tag.Success then {
      return 0
    } else {}

    {
      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Location
      instruction.Location.source_range = source_range.*
    }

    {
      mod_reg_rm := MASS.mod_reg_rm(temp_lhs_register, temp_rhs_register)

      instruction : MASS.Instruction
      instruction.tag = MASS.Instruction_Tag.Bytes
      instruction.Bytes.memory.0 = cast(i8, 0x48)
      instruction.Bytes.memory.1 = cast(i8, 0x39)
      instruction.Bytes.memory.2 = cast(i8, mod_reg_rm)
      instruction.Bytes.length = cast(i8, 3)
      MASS.push_instruction(&builder.code_block, instruction)
    }

    MASS.register_release(builder, temp_rhs_register)
    MASS.register_release(builder, temp_lhs_register)


    value : &MASS.Value = allocate(context.allocator, MASS.Value)
    value.source_range = source_range.*
    value.descriptor = bool
    value.storage.tag = MASS.Storage_Tag.Eflags
    value.storage.bit_size = value.descriptor.bit_size
    value.storage.Eflags.compare_type = MASS.Compare_Type.Unsigned_Below
    value
  }

  payload := allocate(context.allocator, Payload)
  payload.* = [arguments.values.0, arguments.values.1]

  lazy_value := allocate(context.allocator, MASS.Lazy_Value)
  lazy_value.epoch = parser.epoch
  lazy_value.descriptor = bool
  lazy_value.proc = lazy_value_proc
  lazy_value.payload = payload

  meta.static_value(context.compilation, lazy_value, arguments.source_range)
}

unsigned :: module {
  less :: unsigned_less
}

memory_equal :: fn(x, y : x) -> (bool) MASS.generic_equal
memory_not_equal :: fn(x, y : x) -> (bool) MASS.generic_not_equal
equal :: memory_equal
not_equal :: memory_not_equal
// TODO :CompileTimeFnInJitMode
//      Because `=>` fns are allowed to match for jit calls we have to define
//      a separate name for `generic_equal` and call it here, otherwise the code
//      would infinite loop. There are a few solutions to this:
//      1) Keep everything as this, but add a helper that would filter out `=>` overloads:
//           equal :: fn(x, y : x) => (bool) { runtime_overloads(equal)(x, y) }
//      2) Change `=>` semantics to require static storage values. This would resolve
//         the ambiguity but if a programmer wants to have the same code run for static
//         storage values and runtime ones they would have to define a runtime version
//         and then also define a wrapper - either manually or maybe via a macro / intrinsic
equal :: fn(x, y : x) => (bool) { memory_equal(x, y) }
not_equal :: fn(x, y : x) => (bool) { memory_not_equal(x, y) }

add :: MASS.i64_add
subtract :: MASS.i64_subtract
signed_multiply :: MASS.i64_signed_multiply
unsigned_multiply :: MASS.i64_unsigned_multiply
signed_divide :: MASS.i64_signed_divide
unsigned_divide :: MASS.i64_unsigned_divide
signed_remainder :: MASS.i64_signed_remainder
unsigned_remainder :: MASS.i64_unsigned_remainder

signed_less :: MASS.i64_signed_less
unsigned_less :: MASS.i64_unsigned_less
signed_less_equal :: MASS.i64_signed_less_equal
unsigned_less_equal :: MASS.i64_unsigned_less_equal
signed_greater :: MASS.i64_signed_greater
unsigned_greater :: MASS.i64_unsigned_greater
signed_greater_equal :: MASS.i64_signed_greater_equal
unsigned_greater_equal :: MASS.i64_unsigned_greater_equal

negate :: fn(x : i64) => (i64) { 0 - x }

logical_shift_left :: MASS.i64_logical_shift_left
logical_shift_right :: MASS.i64_logical_shift_right
bitwise_or :: MASS.i64_bitwise_or
bitwise_and :: MASS.i64_bitwise_and

using {
  Bits :: MASS.Bits

  module {
    // FIXME there should be better way to provide conversions
    //       instead of combinatorial explosion of overloads

    // It is is bit awkward that the field is called `as_u64` but the type is actually `i64`
    less :: fn(x : Bits, y : Bits) => (bool) { unsigned_less(x.as_u64, y.as_u64) }
    less :: fn(x : Bits, y : i64) => (bool) { unsigned_less(x.as_u64, y) }
    less :: fn(x : i64, y : Bits) => (bool) { unsigned_less(x, y.as_u64) }

    less_equal :: fn(x : Bits, y : Bits) => (bool) { unsigned_less_equal(x.as_u64, y.as_u64) }
    less_equal :: fn(x : Bits, y : i64) => (bool) { unsigned_less_equal(x.as_u64, y) }
    less_equal :: fn(x : i64, y : Bits) => (bool) { unsigned_less_equal(x, y.as_u64) }

    greater :: fn(x : Bits, y : Bits) => (bool) { unsigned_greater(x.as_u64, y.as_u64) }
    greater :: fn(x : Bits, y : i64) => (bool) { unsigned_greater(x.as_u64, y) }
    greater :: fn(x : i64, y : Bits) => (bool) { unsigned_greater(x, y.as_u64) }

    greater_equal :: fn(x : Bits, y : Bits) => (bool) { unsigned_greater_equal(x.as_u64, y.as_u64) }
    greater_equal :: fn(x : Bits, y : i64) => (bool) { unsigned_greater_equal(x.as_u64, y) }
    greater_equal :: fn(x : i64, y : Bits) => (bool) { unsigned_greater_equal(x, y.as_u64) }
  }
}

Allocator :: MASS.Allocator

allocate :: macro(allocator : &MASS.Allocator, type : Type) {
  cast(&type, MASS.allocator_allocate_bytes(
    allocator,
    unsigned_divide(type.bit_size.as_u64, 8),
    unsigned_divide(type.bit_alignment.as_u64, 8)
  ))
}

Array :: fn(item_type : Type, length : i64) -> (Type) intrinsic {
  meta :: import("std/meta")
  item_type := meta.reify(arguments.values.0, Type).*
  length := meta.reify(arguments.values.1, i64).*
  t := allocate(context.allocator, MASS.Descriptor)

  raw_bit_size : i64 = unsigned_multiply(item_type.bit_size.as_u64, length)
  if raw_bit_size & 0xffff_ffff_8000_0000 != 0 then {
    error : MASS.Error
    error.tag = MASS.Error_Tag.Integer_Range
    error.source_range = arguments.source_range
    meta.context_error(context, error)
    return 0
  } else {}

  t.tag = MASS.Descriptor_Tag.Fixed_Size_Array
  t.bit_size = [raw_bit_size]
  t.bit_alignment = item_type.bit_alignment
  t.Fixed_Size_Array = [item_type, length]

  meta.immediate(context.compilation, t, arguments.source_range)
}

multiply :: Array

external :: fn(
  library_name : String,
  symbol_name : String
) => (MASS.External_Symbol) {
  [library_name, symbol_name]
}

os :: MASS.Os
get_target_os :: fn() -> (os._Type) intrinsic {
  meta :: import("std/meta")
  meta.immediate(context.compilation, context.program.os, arguments.source_range)
}

syscall :: fn(number : i64) => (MASS.Syscall) { [number] }

